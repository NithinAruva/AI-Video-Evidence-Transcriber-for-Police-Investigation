{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c71dad4-48ea-4ac1-b799-1ec0eefb58a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: /dbfs/FileStore/final_report.txt\nRead 3576 characters from TXT file.\nCreated 2 chunks.\nTotal chunks created: 2\nDelta table 'character_chunks_group1' for file '/dbfs/FileStore/final_report.txt' created successfully with 2 chunks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Set the current catalog and schema\n",
    "spark.sql(\"USE CATALOG main\")\n",
    "spark.sql(\"USE SCHEMA default\")\n",
    "\n",
    "# Define the path to the single file\n",
    "file_path =  \"/dbfs/FileStore/final_report.txt\"\n",
    "\n",
    "# Helper function for reading different file formats with error handling\n",
    "def read_file(file_path):\n",
    "    try:\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        if file_path.endswith(\".txt\"):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(f\"Read {len(content)} characters from TXT file.\")\n",
    "                return content\n",
    "        elif file_path.endswith(\".pdf\"):\n",
    "            reader = PdfReader(file_path)\n",
    "            text = \"\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "            print(f\"Extracted {len(text)} characters from PDF file.\")\n",
    "            return text\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            doc = Document(file_path)\n",
    "            text = \" \".join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n",
    "            print(f\"Extracted {len(text)} characters from DOCX file.\")\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Character-based chunking with error handling\n",
    "def character_based_chunking(text, chunk_size=2000, overlap=100):\n",
    "    try:\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        text_length = len(text)\n",
    "\n",
    "        while start < text_length:\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            start = end - overlap  # Move the start index with overlap\n",
    "\n",
    "        print(f\"Created {len(chunks)} chunks.\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error in chunking the text: {e}\")\n",
    "\n",
    "# Process the single file and save chunks into a table with error handling\n",
    "def process_single_file(file_path, chunk_size=2000, overlap=100, table_name=\"character_chunks_group1\"):\n",
    "    try:\n",
    "        # Read the document\n",
    "        text = read_file(file_path)\n",
    "\n",
    "        if not text.strip():\n",
    "            print(f\"The file '{file_path}' is empty or contains only whitespace.\")\n",
    "            return\n",
    "\n",
    "        # Perform chunking\n",
    "        chunks = character_based_chunking(text, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "        if not chunks:\n",
    "            print(\"No chunks were created from the file.\")\n",
    "            return\n",
    "\n",
    "        # Create a list of (chunk_id, chunk_text)\n",
    "        chunk_data = [{'chunk_id': i+1, 'chunk_text': chunk} for i, chunk in enumerate(chunks)]\n",
    "        num_chunks = len(chunk_data)  # Count the number of chunks\n",
    "\n",
    "        # Define schema for DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"chunk_id\", IntegerType(), False),\n",
    "            StructField(\"chunk_text\", StringType(), False)\n",
    "        ])\n",
    "\n",
    "        # Create DataFrame\n",
    "        df_chunks = spark.createDataFrame(chunk_data, schema=schema)\n",
    "\n",
    "        # Save DataFrame as a table with overwriteSchema option to resolve schema conflicts\n",
    "        df_chunks.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n",
    "\n",
    "        # Enable Change Data Feed on the Delta table\n",
    "        spark.sql(f\"ALTER TABLE `main`.`default`.`{table_name}` SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "        # Print the number of chunks created and success message\n",
    "        print(f\"Total chunks created: {num_chunks}\")\n",
    "        print(f\"Delta table '{table_name}' for file '{file_path}' created successfully with {num_chunks} chunks.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process file '{file_path}': {e}\")\n",
    "\n",
    "# Example usage: Process the single file\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Process the single file with default chunk_size and overlap\n",
    "        process_single_file(file_path)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"Input Error: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ca480e-6ec2-4425-8e9b-31fe55ce7233",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ['DATABRICKS_TOKEN'] = 'dapif26092496687d2f91846997b3528fdcc-3'\n",
    "index_name=\"main.default.vsi_char_group123\"\n",
    "VECTOR_SEARCH_ENDPOINT_NAME=\"pavankumar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09224be-ced9-4e10-968d-5cf9831a18f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from langchain_community.embeddings import DatabricksEmbeddings\n",
    "\n",
    "# Setup embeddings model\n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-e\")\n",
    "\n",
    "def get_retriever():\n",
    "    os.environ[\"DATABRICKS_HOST\"] = host\n",
    "\n",
    "    # Initialize Vector Search Client\n",
    "    vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ[\"DATABRICKS_TOKEN\"])\n",
    "\n",
    "    # Get vector search index\n",
    "    vs_index = vsc.get_index(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "        index_name=index_name\n",
    "    )\n",
    "\n",
    "    # Create the DatabricksVectorSearch object\n",
    "    vectorstore = DatabricksVectorSearch(\n",
    "        vs_index, text_column=\"chunk_text\", embedding=embedding_model\n",
    "    )\n",
    "    \n",
    "    # Return retriever object\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baefc7fe-4fae-444f-bbec-c81141554c94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a Personal Authentication Token (PAT). Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.vectorstores.databricks_vector_search:embedding model is not used in delta-sync index with Databricks-managed embeddings.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from langchain_community.embeddings import DatabricksEmbeddings\n",
    "import os\n",
    "\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ['DATABRICKS_TOKEN'] = 'dapif26092496687d2f91846997b3528fdcc-3'\n",
    "index_name=\"main.default.vsi_char_group123\"\n",
    "VECTOR_SEARCH_ENDPOINT_NAME=\"pavankumar\"\n",
    "\n",
    "def get_retriever():\n",
    "    os.environ[\"DATABRICKS_HOST\"] = host\n",
    "    vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ[\"DATABRICKS_TOKEN\"])\n",
    "    vs_index = vsc.get_index(endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=index_name)\n",
    "    vectorstore = DatabricksVectorSearch(vs_index, text_column=\"chunk_text\", embedding=embedding_model)\n",
    "    return vectorstore.as_retriever()\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-dbrx-instruct\", max_tokens = 200)\n",
    "\n",
    "TEMPLATE = \"\"\"You are an assistant specialized in police investigations. You are answering questions related to police reports, criminal activities, suspects, and investigations based on the information provided in the PDF documents. \n",
    "If the question is not related to these topics, kindly decline to answer. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "If the question appears to be for a report or document you don't have data on, say so. \n",
    "Provide all answers only in English, and ensure the responses are concise, relevant, and factual.\n",
    "Use the following pieces of context to answer the question at the end:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=get_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7ef86b-fe02-4181-b774-178be6f95da4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "question = dbutils.widgets.get(\"query\")\n",
    "# query=\"hello\"\n",
    "answer = chain.run(question)\n",
    "dbutils.notebook.exit(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c547a5c-cd78-4a75-9731-05b48e018fe1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "QA_bot",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
